{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "import pandas as pd\n",
    "from colorcet import fire\n",
    "from datashader import transfer_functions as tf\n",
    "from datashader.utils import lnglat_to_meters as webm\n",
    "import pickle\n",
    "import reverse_geocoder as rg\n",
    "import altair as alt\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "Path.cont = lambda x: list(os.scandir(x))\n",
    "import logging\n",
    "import contextlib\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "@contextlib.contextmanager\n",
    "def timeit(ident):\n",
    "    tstart = time.time()\n",
    "    yield\n",
    "    elapsed = (time.time() - tstart)/60\n",
    "    logging.info(f'{ident}, time: {elapsed}')\n",
    "\n",
    "def inspect_na(df):\n",
    "    print(df.isnull().sum(axis=1).value_counts(sort=False))\n",
    "    df_len = len(df)\n",
    "    df = df.isna().copy()\n",
    "    df = df.loc[:, (df != 0).any(axis=0)]\n",
    "    df = df.loc[(df!=0).any(1)]\n",
    "    df_nan_dict = {c : [len(df.loc[df[c] & df[c2]]) for c2 in df.columns] for c in df.columns}\n",
    "    df_nan = pd.DataFrame.from_dict(df_nan_dict)\n",
    "    df_nan.set_index(df.columns, inplace=True)\n",
    "    df_nan.insert(0, 'total', df_nan.max(axis=0))\n",
    "    df_nan.sort_values('total', inplace=True, ascending=False)\n",
    "    df_nan = pd.concat([df_nan['total'] , df_nan.iloc[:,1:][list(df_nan.index)]],axis=1)\n",
    "    return (df_nan/df_len).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dates(df, name):\n",
    "    df['date'] = name[:10]\n",
    "    df['hour'] = name[11:13]\n",
    "    return df\n",
    "\n",
    "def reverse_geocode(df):\n",
    "    # many thanks to https://github.com/thampiman/reverse-geocoder\n",
    "    coor = tuple((lat,lon) for lat, lon in zip(df['lat'],df['lon']))\n",
    "    res = rg.search(coor)\n",
    "    print(res[0])\n",
    "    df['country'] = [i['cc'] for i in res]\n",
    "    df['provincie'] = [i['admin1'] for i in res]\n",
    "    df['city'] = [i['name'] for i in res]\n",
    "    return df\n",
    "\n",
    "def process_files(path):\n",
    "    dataframes = []\n",
    "    for direntry in path.cont():\n",
    "        if direntry.name.endswith('pickle'):\n",
    "            f = open(direntry,\"rb\")\n",
    "            df = pickle.load(f)\n",
    "            df = add_dates(df, direntry.name)\n",
    "            print(len(df))\n",
    "            dataframes.append(df)\n",
    "    return pd.concat(dataframes)\n",
    "\n",
    "def project_gps(df):\n",
    "    # Project longitude and latitude onto web mercator plane.\n",
    "    df.loc[:, 'easting'], df.loc[:, 'northing'] = webm(df['lon'],df['lat'])\n",
    "    return df\n",
    "\n",
    "def categorify(df):\n",
    "    categorical = ['provincie', 'country', 'hour', 'date']\n",
    "    for cat in categorical: df[cat] = df[cat].astype('category')\n",
    "    # categorize barometric altitude\n",
    "    bins = [-1000, 2000, 10000, 100000000]\n",
    "    names = ['below 2000', 'below FL100', 'above FL100']\n",
    "    df['altitude'] = pd.cut(df['baroaltitude'], bins, labels=names)\n",
    "    print(df['altitude'].value_counts())\n",
    "\n",
    "    # categorize ascending/descending\n",
    "    bins = [-4000000, -2, 2, 400000]\n",
    "    names = ['descending', 'stable', 'ascending']\n",
    "    df['mode'] = pd.cut(df['vertrate'], bins, labels=names)\n",
    "    print(df['mode'].value_counts())\n",
    "    return df\n",
    "\n",
    "def save_df(df,path):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with timeit('Reading individual files files'):    \n",
    "    df = process_files(Path(r'C:\\Users\\Jesse\\Documents\\GitHub\\projects\\flight_data_plotter\\opensky_scrape'))\n",
    "with timeit('Reversing geocodes'):    \n",
    "    f = reverse_geocode(df) # may take a while\n",
    "df = project_gps(df)\n",
    "df = categorify(df)\n",
    "save_df(df,Path(r'C:\\Users\\Jesse\\Documents\\GitHub\\projects\\flight_data_plotter\\opensky_scrape\\flights.db'))\n",
    "print('Done, total records', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(Path(r'C:\\Users\\Jesse\\Documents\\GitHub\\projects\\flight_data_plotter\\opensky_scrape\\flights.db'),\"rb\")\n",
    "df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Removing: keep only NL, 3698863, 2292411, proportion 0.38023900858182635\nRemoving: NaN for baro altitude, 2292411, 2277743, proportion 0.0063985035842176645\nRemoving: NaN for velocity, 2277743, 2242502, proportion 0.015471894766003013\nRemoving: NaN for callsign, 2242502, 2240289, proportion 0.0009868441588903824\nRemoving: NaN for squawk, 2240289, 2237332, proportion 0.001319918992594259\n0    2152031\n1      85301\ndtype: int64\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             total  geoaltitude\ngeoaltitude   0.04         0.04",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total</th>\n      <th>geoaltitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>geoaltitude</th>\n      <td>0.04</td>\n      <td>0.04</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "def pd_remove(df, condition,description = None):\n",
    "    len_before = len(df)\n",
    "    df = df.loc[condition]\n",
    "    len_after = len(df)\n",
    "    print(f'Removing: {description}, {len_before}, {len_after}, proportion {(len_before-len_after)/len_before}') \n",
    "    return df\n",
    "\n",
    "# drop other countries\n",
    "df = pd_remove(df, df['country'] == 'NL', 'keep only NL')\n",
    "df = pd_remove(df, df['baroaltitude'].notna(), 'NaN for baro altitude')\n",
    "df = pd_remove(df, df['velocity'].notna(), 'NaN for velocity')\n",
    "df = pd_remove(df, df['callsign'].notna(), 'NaN for callsign')\n",
    "df = pd_remove(df, df['squawk'].notna(), 'NaN for squawk')\n",
    "\n",
    "inspect_na(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:root:sorting by icao24, time: 0.17105214595794677\n2237332\n19014\n38028\nINFO:root:inserting after callsign, time: 7.137673207124075\n2210084\ndone total records 2210084\n"
    }
   ],
   "source": [
    "empty = df[:1].reset_index()\n",
    "empty[:1] = np.NaN*26\n",
    "\n",
    "def insert_nan_rows(df, iterable):\n",
    "    print(len(df))\n",
    "    groups = [[groups,empty] for groups in iterable]\n",
    "    print(len(groups))\n",
    "    groups = [item for i in groups for item in i]\n",
    "    print(len(groups))\n",
    "    test = pd.concat(groups, ignore_index=True)\n",
    "    print(len(test))\n",
    "    return test\n",
    "\n",
    "with timeit('sorting by icao24'):\n",
    "    df2 = df.sort_values(['icao24','lastposupdate'])\n",
    "\n",
    "groups = [split_df for split_df in np.split(df2.reset_index(drop=True), np.where(df2['lastposupdate'].diff() > 120)[0]) if len(split_df) > 20 ]\n",
    "with timeit('inserting after callsign'):\n",
    "    df2 = insert_nan_rows(df2, groups)\n",
    "\n",
    "# assert len(df2.loc[df2['callsign']=='ZXP26   ']) == len(df.loc[df['callsign']=='ZXP26   '])\n",
    "\n",
    "p = Path(r'C:\\Users\\Jesse\\Documents\\GitHub\\projects\\flight_data_plotter\\opensky_scrape\\flights_preprocessed_NL.db')\n",
    "with open(p, 'wb') as handle:\n",
    "    pickle.dump(df2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print('done total records', len(df2))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(r'C:\\Users\\Jesse\\Documents\\GitHub\\projects\\flight_data_plotter\\opensky_scrape\\flights_preprocessed.db')\n",
    "f = open(p),\"rb\")\n",
    "df = pickle.load(f)\n",
    "\n",
    "x_range = (min(df['easting']),max(df['easting']))\n",
    "y_range = (min(df['northing']),max(df['northing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that we have a variety of columns with data about each of the 10 million taxi trips here, such as the locations in Web Mercator coordinates, the distance, etc.  With datashader, we can choose what we want to plot on the `x` and `y` axes and see the full data immediately, with no parameter tweaking, magic numbers, subsampling, or approximation, up to the resolution of the display:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "kernelspec": {
   "name": "python37364bitbaseconda64cd2d4475b7431fad90d3a78fe97c82",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}